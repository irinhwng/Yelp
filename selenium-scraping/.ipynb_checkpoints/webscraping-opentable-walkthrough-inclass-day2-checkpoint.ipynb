{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Webscraping OpenTable with Selenium: Guided Lab\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> *Note: this lab is intended to be instructor guided.*\n",
    "\n",
    "\n",
    "In today's codealong lab, we will build a scraper using urllib and BeautifulSoup. We will remedy some of the pitfalls of automated scraping by using a a \"headless\" browser called Selenium.\n",
    "\n",
    "You will be scraping OpenTable's Austin listings. We're interested in knowing the restaurant's **name, location, price, and how many people booked it today.**\n",
    "\n",
    "OpenTable provides all of this information on this given page: [Open table listings](https://www.opentable.com/austin-restaurant-listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspect the elements of this page to assure we can find each of the bits of information in which we're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use `requests` and `BeautifulSoup` to read the contents of the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "url = 'https://www.opentable.com/austin-restaurant-listings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the url we want to visit\n",
    "res = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Beautiful Soup to convert the raw HTML into a soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract the name of each restaurant.\n",
    "\n",
    "Let's first find each restaurant name listed on the page we've loaded. How do we find the page location of the restaurant? \n",
    "\n",
    "> *Hint: we need to know where in the **html** the restaurant element is housed.*\n",
    "\n",
    "**4.A See if you can find the restaurant name on the page. Keep in mind there are many restaurants loaded on the page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dixie Stracke\n",
      "Georgiannas\n",
      "Quia Vista\n",
      "1327 Bergnaum\n",
      "Views\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('span', {'class':'rest-row-name-text'})[:5]:\n",
    "    print(i.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.B See any issues here?.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Enter Selenium - resolve the javascript issue using the driver and find the bookings.\n",
    "\n",
    "What we can do in this case is:\n",
    "1. Request that the page load\n",
    "2. wait one second\n",
    "3. grab the source html from the page \n",
    "\n",
    "Because the page should believe I'm visiting from a live connection on a browser client, the JavaScript should render to be a part of the page source. I can then grab the page source.\n",
    "\n",
    "**Once you have the HTML with the javascript rendered, repeat the processes above.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# uncomment below for macos\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/macos/chromedriver\") \n",
    "\n",
    "#uncomment below for windows \n",
    "#  driver = webdriver.Chrome(executable_path=\"./chromedriver/windows/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Grove Wine Bar and Kitchen Downtown\n",
      "Uchiko\n",
      "Perry's Steakhouse & Grille - Downtown Austin\n",
      "The Salty Sow\n",
      "Rosewood\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "\n",
    "for i in soup.find_all('span', {'class':'rest-row-name-text'})[:5]:\n",
    "    print(i.text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repeat the process above, and let's grab location as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downtown\n",
      "Central Austin\n",
      "Downtown\n",
      "East Austin\n",
      "East Austin\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('span', {'class':'rest-row-meta--location rest-row-meta-text'})[:5]:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get the price for each restaurant.\n",
    "\n",
    "The price is number of dollar signs on a scale of one to four for each restaurant. We'll follow the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('i', {'class':'pricing--the-price'})[:5]:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.B Convert the dollar sign strings to a count of the number of dollar signs.**\n",
    "\n",
    "Can you figure out a way to simply print out the number of dollar signs per restaurant listed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "for i in soup.find_all('i', {'class':'pricing--the-price'})[:5]:\n",
    "    print(i.text.count('$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Can you find the number of times a restaurant was booked.\n",
    "\n",
    "In the next cell, print out a sample of objects that contain the number of times the restaurant was booked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sleep\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "68\n",
      "77\n",
      "55\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# A: grabbing the bookings \n",
    "\n",
    "for i in soup.find_all('div', {'class': 'booking'})[:5]:\n",
    "    print(i.text.split(' ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Can we get all of the items we want from the page in a single `find_all`?\n",
    "\n",
    "To be most efficient, we want to only do a single loop for each entry on the page. That means we want to find what element all of other other elements (name, location, price, bookings) is housed within. Where on the page is each entry located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Grove Wine Bar and Kitchen Downtown\n",
      "Downtown\n",
      "2\n",
      "19\n",
      "Uchiko\n",
      "Central Austin\n",
      "3\n",
      "68\n",
      "Perry's Steakhouse & Grille - Downtown Austin\n",
      "Downtown\n",
      "3\n",
      "77\n",
      "The Salty Sow\n",
      "East Austin\n",
      "2\n",
      "55\n",
      "Rosewood\n",
      "East Austin\n",
      "3\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('div', {'class':'rest-row-info'})[:5]:\n",
    "    print(i.find('span', {'class':'rest-row-name-text'}).text)\n",
    "    print(i.find('span', {'class':'rest-row-meta--location rest-row-meta-text'}).text)\n",
    "    print(i.find('i', {'class':'pricing--the-price'}).text.count('$'))\n",
    "    try:\n",
    "        print(i.find('div', {'class':'booking'}).text.split(' ')[1])\n",
    "    except:\n",
    "        (print('0 bookings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Does every single entry have each element we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Use python exceptions to handle cases when bookings aren't found.\n",
    "\n",
    "When a booking is not found, store 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Putting it all together in a dataframe.\n",
    "\n",
    "**Loop through each entry. For each entry:**\n",
    "1. Grab the relevant information we want (name, location, price, bookings). \n",
    "2. Produce a dataframe with the columns \"name\",\"location\",\"price\",\"bookings\" that contains the 100 entries we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Making this a function\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'location', 'price', 'bookings'])\n",
    "\n",
    "for page_number in range(5):\n",
    "    sleep(2)\n",
    "    if page_number == 0:\n",
    "        driver = webdriver.Chrome(executable_path=\"./chromedriver/macos/chromedriver\")\n",
    "        driver.implicitly_wait(5)\n",
    "        driver.get('https://www.opentable.com/chicago-restaurant-listings')\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    else:\n",
    "        next_button = driver.find_element_by_link_text('Next')\n",
    "        next_button.click()\n",
    "\n",
    "    for i in soup.find_all('div', {'class':'rest-row-info'})[:]:\n",
    "        name = i.find('span', {'class':'rest-row-name-text'}).text\n",
    "        location = i.find('span', {'class':'rest-row-meta--location rest-row-meta-text'}).text\n",
    "        price = i.find('i', {'class':'pricing--the-price'}).text.count('$')\n",
    "        try:\n",
    "            bookings = i.find('div', {'class':'booking'}).text.split(' ')[1]\n",
    "        except:\n",
    "            bookings = '0 bookings'\n",
    "\n",
    "        df.loc[len(df)] = [name, location, price, bookings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add together all the pieces here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Use selenium to loop through at least 5 pages and grab that information as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "The above example (and many others) are available in the Selenium docs: http://selenium-python.readthedocs.io/getting-started.html\n",
    "\n",
    "What is especially important is exploring functionality like locating elements: http://selenium-python.readthedocs.io/locating-elements.html#locating-elements\n",
    "\n",
    "FAQ:\n",
    "http://selenium-python.readthedocs.io/faq.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
